{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrWjtJzGtnSS"
      },
      "outputs": [],
      "source": [
        "pip install torchtune"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tune"
      ],
      "metadata": {
        "id": "4hY4FvVVtuWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn,Tensor\n",
        "\n",
        "class LORALinear(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      in_dim: int,\n",
        "      out_dim: int,\n",
        "      rank:int,\n",
        "      alpha: float,\n",
        "      dropout: float,\n",
        "  ):\n",
        "  #weights from original pretrained model\n",
        "    self.linear = nn.Linear(in_dim,out_dim,bias=False)\n",
        "\n",
        "    #new Lora parameters. in general rank<<in_dim, out_dim\n",
        "    self.lora_a = nn.Linear(in_dim,rank,bias=False)\n",
        "    self.lora_b = nn.Linear(rank,out_dim,bias=False)\n",
        "\n",
        "    #rank and alpha are commonly tuned hyperparameters\n",
        "    self.rank= rank\n",
        "    self.alpha = alpha\n",
        "\n",
        "    #dropout\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    #original parameters are frozen and only lora parameters are trained\n",
        "    self.linear.weight.requires_grad = False\n",
        "    self.lora_a.weight.requires_grad = True\n",
        "    self.lora_b.weight.requires_grad = True\n",
        "\n",
        "  def forward(self, x:Tensor) ->Tensor:\n",
        "    #output of the original model\n",
        "    frozen_out = self.linear(x)\n",
        "\n",
        "    #lora_a projects inputs down to the much smaller self.rank\n",
        "    #then lora_b projects back to the output dimension\n",
        "\n",
        "    lora_out = self.lora_b(self.lora_a(self.dropout(x)))\n",
        "\n",
        "    #finally scale by alpha parameter (normalized by rank)\n",
        "    # and add to the original model's output\n",
        "\n",
        "    return frozen_out + (self.alpha/self.rank)*lora_out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z84G1bzBuAlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wX1d2FYIzAWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applying Lora to Phi 3 model"
      ],
      "metadata": {
        "id": "vqaVHHFQzFyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "iBnY8p_K6kZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hHI9erQ16l2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30522\n",
        "num_layers = 6\n",
        "num_heads = 8\n",
        "num_kv_heads = 1\n",
        "embed_dim = 512\n",
        "intermediate_dim = 2048\n",
        "max_seq_len = 512"
      ],
      "metadata": {
        "id": "CXWHxTug7WZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtune.models.phi3 import phi3, lora_phi3\n",
        "#llama2 without lora layers\n",
        "base_model = phi3(vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_kv_heads=num_kv_heads,\n",
        "        embed_dim=embed_dim,\n",
        "        intermediate_dim=intermediate_dim,\n",
        "        max_seq_len=max_seq_len).to(device)\n",
        "\n",
        "# The default settings for lora_llama2_7b will match those for llama2_7b\n",
        "# We just need to define which layers we want LoRA applied to.\n",
        "# Within each self-attention, we can choose from [\"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\"].\n",
        "# We can also set apply_lora_to_mlp=True or apply_lora_to_output=True to apply LoRA to other linear\n",
        "# layers outside of the self-attention.\n",
        "\n"
      ],
      "metadata": {
        "id": "kKQGa3zgzJKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_model = lora_phi3(vocab_size=vocab_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads,\n",
        "        num_kv_heads=num_kv_heads,\n",
        "        embed_dim=embed_dim,\n",
        "        intermediate_dim=intermediate_dim,\n",
        "        max_seq_len=max_seq_len,\n",
        "        lora_attn_modules=['q_proj','v_proj'],\n",
        "        lora_rank=8,\n",
        "        lora_alpha=16)"
      ],
      "metadata": {
        "id": "pnhmnDpz6NLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_model.layers[0])"
      ],
      "metadata": {
        "id": "nZA7_RBH0YK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4899d3a-df8e-4826-96de-755ca8f12b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerDecoderLayer(\n",
            "  (sa_norm): RMSNorm()\n",
            "  (attn): CausalSelfAttention(\n",
            "    (q_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (k_proj): Linear(in_features=512, out_features=64, bias=False)\n",
            "    (v_proj): Linear(in_features=512, out_features=64, bias=False)\n",
            "    (output_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (pos_embeddings): Phi3RotaryPositionalEmbeddings()\n",
            "  )\n",
            "  (mlp_norm): RMSNorm()\n",
            "  (mlp): FeedForward(\n",
            "    (w1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "    (w2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "    (w3): Linear(in_features=512, out_features=2048, bias=False)\n",
            "    (activation): SiLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(lora_model.layers[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfFCYFHq8Ddu",
        "outputId": "95b14031-f99f-4427-e5e8-1d66fef5254f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerDecoderLayer(\n",
            "  (sa_norm): RMSNorm()\n",
            "  (attn): CausalSelfAttention(\n",
            "    (q_proj): LoRALinear(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (lora_a): Linear(in_features=512, out_features=8, bias=False)\n",
            "      (lora_b): Linear(in_features=8, out_features=512, bias=False)\n",
            "    )\n",
            "    (k_proj): Linear(in_features=512, out_features=64, bias=False)\n",
            "    (v_proj): LoRALinear(\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (lora_a): Linear(in_features=512, out_features=8, bias=False)\n",
            "      (lora_b): Linear(in_features=8, out_features=64, bias=False)\n",
            "    )\n",
            "    (output_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (pos_embeddings): Phi3RotaryPositionalEmbeddings()\n",
            "  )\n",
            "  (mlp_norm): RMSNorm()\n",
            "  (mlp): FeedForward(\n",
            "    (w1): Linear(in_features=512, out_features=2048, bias=False)\n",
            "    (w2): Linear(in_features=2048, out_features=512, bias=False)\n",
            "    (w3): Linear(in_features=512, out_features=2048, bias=False)\n",
            "    (activation): SiLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#assuming that base model has pretrained weights\n",
        "#this will load them directly into lora model without any conversion necessary\n",
        "\n",
        "lora_model.load_state_dict(base_model.state_dict(),strict=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdsc8SJm8G1M",
        "outputId": "f833af5f-2a77-4ac5-f3ec-7f302773188a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=['layers.0.attn.q_proj.lora_a.weight', 'layers.0.attn.q_proj.lora_b.weight', 'layers.0.attn.v_proj.lora_a.weight', 'layers.0.attn.v_proj.lora_b.weight', 'layers.1.attn.q_proj.lora_a.weight', 'layers.1.attn.q_proj.lora_b.weight', 'layers.1.attn.v_proj.lora_a.weight', 'layers.1.attn.v_proj.lora_b.weight', 'layers.2.attn.q_proj.lora_a.weight', 'layers.2.attn.q_proj.lora_b.weight', 'layers.2.attn.v_proj.lora_a.weight', 'layers.2.attn.v_proj.lora_b.weight', 'layers.3.attn.q_proj.lora_a.weight', 'layers.3.attn.q_proj.lora_b.weight', 'layers.3.attn.v_proj.lora_a.weight', 'layers.3.attn.v_proj.lora_b.weight', 'layers.4.attn.q_proj.lora_a.weight', 'layers.4.attn.q_proj.lora_b.weight', 'layers.4.attn.v_proj.lora_a.weight', 'layers.4.attn.v_proj.lora_b.weight', 'layers.5.attn.q_proj.lora_a.weight', 'layers.5.attn.q_proj.lora_b.weight', 'layers.5.attn.v_proj.lora_a.weight', 'layers.5.attn.v_proj.lora_b.weight'], unexpected_keys=[])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#once weights are loaded set lora parameters to trainable\n",
        "from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params\n",
        "\n",
        "#fetch all params from the model that are associated with lora\n",
        "lora_params = get_adapter_params(lora_model)\n",
        "\n",
        "#set requires_grad =True on lora params and requires_grad=False on all other\n",
        "set_trainable_params(lora_model,lora_params)\n",
        "\n",
        "#print total number of params\n",
        "total_params = sum([p.numel() for p in lora_model.parameters()])\n",
        "trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\n",
        "\n",
        "print(f\"\"\"{total_params} total parameters,\n",
        " {trainable_params} trainable parameters,\n",
        "  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJCy04Ea928Y",
        "outputId": "f66e2ab2-603a-4ed0-b1a4-13b10b659068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53751296 total parameters,\n",
            " 76800 trainable parameters,\n",
            "  0.14% of all params are trainable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXvoC8Dv922I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}